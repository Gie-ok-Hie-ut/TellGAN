\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Project Report\\
TellGAN: Motion Prediction and Video Generation From Natural Text}

\author{Gunhee Lee, Jacob Morton\\
20172811, 20172327\\
Computer Science and Engineering, POSTECH\\
{\tt\small victorleee@postech.ac.kr}
}


\maketitle
%\thispagestyle{empty}


%%%%%%%%% BODY TEXT
\section{Abstract}

 We propose a Tell Generative Adversarial Network (TellGAN) which, given an initial starting frame with landmarks and text that describes an action, will generate plausible video frames of the given target performing the action. We call it TellGAN, because we tell the target what to do. Our method is a generic end-to-end approach that predicts motion and generates video frames. We divide the task into two parts, landmark trajectory prediction from a given word and generating a video frames from the predicted landmarks and initial target position. We also propose a novel use of an LSTM to predict motion over a various number of frames, to model motions at different speeds. We chose mouth motion prediction for a given word as an application for our model. Although this application is related to lip reading (video-to-text), a well researched area, it appears that text-to-video face motion has not been approached directly before.  Among the several datasets used for lip reading, we used the GRID Corpus\footnote{http://spandh.dcs.shef.ac.uk/gridcorpus/}.
 
%------------------------------------------------------------------------
\section{Introduction}
 Motion Prediction is a well researched area with several existing approaches that predict landmarks using RNN and LSTM networks. Although these networks are able to predict motions given an action, they currently can only learn and predict actions at a constant rate of change, basically learning an average representation. This is a problem in real life, when actions are performed at different rates by different individuals and times. Our model can learn and predict motions performed at various speeds. To our knowledge this problem hasn't been tackled yet. We are able to train an LSTM motion predictor to learn an 'oversampled' representation of the action learned at a higher rate than available from the ground truth. We use a weak loss for predictions with missing ground truths and rely on the dataset to provide various sequence length actions to correctly train the network. We chose a lib-reading dataset to provide a large number of actions with varying rates of speed to train and test our model on text-driven animation.

 Text-Driven animation is not a new topic, but the current approaches use text-to-speech generators to drive animation and are not typically generic. There has been some recent independent techniques developed concurrently with ours that use Generative Adversarial Networks (GAN) for generating video frames, but again use audio as a driving factor. Text-to-speech techniques offer several advantages as there are plenty of previous works and is an easier task to map continuous wave forms rather than discrete words to the motions of the mouth during speech. Our proposed method will skip the text-to-speech generation by training a GAN and an LSTM with the aid of lip-reading techniques. Visual only lip reading is a heavily research topic and is essentially a video-to-text and action classification problem. Our proposed approach is the inverse to lip reading, a Text-to-Video and action generation problem. Generating compelling speech motions from words is a challenging problem. Humans are good at recognizing fine facial movements and can intemperate words from mouth motions. Also most motion prediction techniques don't demonstrate predictions of the fine motions required to model speech.

 Generative adversarial networks have improved in recent years. There also has been research which relates generative adversarial network and natural language processing. Text-to-image generation is one of the most obvious examples of it. Using a GAN to Generate video or images from changing landmarks is currently an active area of research. We use a UNet architecture that takes as input an initial frame and desired landmark positions to change the image to the desired state. The results are of a high degree of realism, where the line between a fake image and real image is blurred.
  
 Compared with the normal image generation model, our model receives the input image. The contemporary state of the art network called AttentionGAN (Tao Xu et al.) can only change the image as a whole to meet the sentence direction, which is not desirable. For example, the minor changes that occure between frames would result in a completely different image. Our model would make the image more controllable than the previous approach. Also our model has a temporal component, as we propose generating a series of images that correspond spoken motions derived from the given input to make a video sequence. This has several challenges, among which are maintaining temporal coherency and preventing errors cascading through future generated frames. Previous work on text-to-video using GAN exists\footnote{Video Generation From Text (Li, 2017)}. We will not use this method, as we will not have an end frame nor do we wish to use simultaneous generation of frame sequences. Instead we will opt for simplicity with a sequential approach that divides the task into separate steps, motion prediciton from action and image generation from landmarks.


%------------------------------------------------------------------------
\section{Dataset}
Since our approach is related to lip-reading, we can use many of the available datasets. We will choose to use the GRID Coprus. The dataset contains 33 individuals each speaking 1000 sentences. Each sentence is composed of 6 words. Video, audio, and timestamped transcripts (word level) are included. The recorded subjects are static in position, uniformly lit, with no expression. Instead of using a large vocabulary range (51 words), GRID tries cover more phonemes instead\footnote{An audio-visual corpus for speech perception and automatic speech recognition (Cooke, 2006)}. The smallest unit of speach is the phoneme. A word can easily converted into a set of phonemes. A viseme is the visual component  of the phoneme, where there is a many to one phoneme-to-viseme mapping. The english language is comprised of 45 phonemes which are mapped to 17 visemes according to the Fisher phoneme-to-viseme mapping\footnote{Confusions among visually perceived consonants (Fisher, 1968)}.


%------------------------------------------------------------------------
\section{Network Architecture}
The proposed architecture uses GAN network model which is comprised of generator and discriminator. The generator has 2 different stages to make image, motion predictor and image generator. We use two different types of discriminators, sequence discriminator, and pair discriminator. The overall architecture looks like this.
\begin{figure*}
\begin{center}
\includegraphics [scale=0.3] {images/network.JPG}
\end{center}
 \caption{The overall architecture of our network.  Generator consists of motion predictor, and image generator. Discriminator consists of sequence discriminator, and pair discriminator}
\label{fig:short}
\end{figure*}


\subsection{Two - Stage Generator}
The generator is comprised of motion predictor and the image generator. Motion predictor gets the condition and previous landmark positions and predicts the next landmark. Image generator gets this predicted landmark and initial photo to make a realistic image based on this landmark.

\subsubsection{Motion Predictor}
Motion predictor predicts the next motion based on the previous landmark information and condition. As we said above, the word information roles as a condition in this project. We first tiled the word information to make this have the same size as the landmark information have. Then we concatenated this word information with the initial landmark information. All the landmark information is represented as a table which represents x, and y value of all points. This concatenated information now feeds into the LSTM network. In the LSTM network
\subsubsection{Image Generator}

Image generator uses an initial image and predicted landmarks. We used U-Net network to modify initial image based on the predicted landmarks. The landmark image and initial image is concatenated and passed through this network. U-Net network is a well-known network architecture which uses skip connections to reconstruct the image. For encoder and decoder part we all used the convolution layers and instance normalization layers. There are many ways for the decoder part to make the image size big. In this time, we used the transpose convolution as an upsampling.

When we trained this model we first trained the image generator with the ground truth landmark at time t to avoid errors from motion predictor. We thought the wrong output from motion predictor could hinder image generator and pair discriminator from converging. After we could sure that both motion predictor and image generator works quite well, we used the predicted landmarks to the image generator.
\subsection{Discriminator}
 There are two different types of a discriminator, sequence discriminator, and pair discriminator.
\subsubsection{Sequence Discriminator}
Sequence discriminator distinguishes between real sequence data and fake sequence data. Real sequence data is comprised of a ground truth initial frame and a current frame. Fake sequence data is comprised of a ground truth initial frame, previous frame, and fake current frame. Unlike other discriminators, this discriminator should get different size of the input at each time. To deal with this problem, we introduced the sequence discriminator which has the LSTM network as a basic structure. We used LSTM network which is said to be good at handling vanishing gradient problem through time.
This sequence discriminator also can be used as a word discriminator. All the concatenated data starts from the word information first. We expect that if we give a fake word at the first time and concatenate the ground truth sequence, the sequence discriminator should determine it as a false. In this paper, we are not using this modified form of sequence discriminator.
\subsubsection{Pair Discriminator}
Pair discriminator distinguishes between real pair data and fake pair data. Real pair data is concatenated by real landmark and real image. Fake pair data is concatenated by real landmark and predicted image from image generator. Real landmark is given as a condition to this network.
\section{Learning Prediction by Up-sampling}
The challenge in this predictor is due to the variety of trajectories of each landmark points. The trajectory could be different every time. This could be varied depends on how long that the word would be spoken, how loud or small the word would be spoken, and what word is said before. We thought the time length of each word mainly affects the difficulties of this problem.
To solve this variation of time problem, we are introducing $Learning$ $ prediction$ $by$ $upsampling$ method. This method assumes that the overall trajectory of each word is similar, and this trajectory is just scaled up with respect to the time length. The motion predictor would predict motion with the fixed number of time which should be larger than the maximum number of ground truth time length of each word. In other words, all the ground truth would be scaled up to some fixed length, and the motion predictor should predict the motion at every fixed length of time.
This approach is based on the Nyquist sampling theorem. If the sampling frequency is at least twice much higher than the original signal frequency, the sample would be sufficient to represent the original signal. The video sampling frequency should be at least twice much higher than the landmark analogous signal. Let's say there is a $N$ number of motion, $M_k$, in set $M = \big[\ M_k|M_1,M_2, ... , M_N\big]$. The video frequency of word $M_k$ can be represented like  $freq_{video}^{M_k}$, and original landmark analogous signal can be represented like $freq_{analog}^{M_k}$.This all can be represented like this using Nyquist sampling theorem.

\bigskip
\begin{center}
$freq_{video}^{M_k} \geq 2 \ast freq_{analog}^{M_k} $
\end{center}
\bigskip

In the same way, the prediction frequency,$freq_{pred}^{M_k}$, also should be at least twice much higher than the landmark analogous signal. This also can be represented like this.

\bigskip
\begin{center}
$freq_{pred}^{M_k} \geq 2 \ast freq_{analog}^{M_k} $
\end{center}
\bigskip

Combining this two equations, the prediction frequency is sufficient to be larger than the max video frequency. Then, prediction frequency would also be at least twice much higher than the landmark analogous signal.

\bigskip
\begin{center}
$freq_{pred}^{M_k} \geq MAX[freq_{analog signal}^{M_k}] $
\end{center}
\bigskip

This method can be represented like Figure 2.

\begin{figure}[h]
\begin{center}
\includegraphics [scale=0.33] {Images/upsample1.JPG}
\end{center}
\caption{Learning Prediction by Up-sampling. $Left:$ Same motion $M_k$ have the different number of frames. $Right:$ This GT value can be scaled up to specific fixed number of frames.}
\label{fig:short}
\label{fig:onecol}
\end{figure}


Every different time length with the same motion is scaled up to the same amount of frame. The motion predictor predicts the motion every time and it is back-propagated every time. We introduced the ‘strong identity loss’ and ‘weak identity loss’. When there is a ground truth value, we adjust strong identity loss to minimize the value. If there is no ground truth value, we perform weak identity loss between predicted value and the next ground truth value. We used the L1 loss to minimize the distance between ground truth position and predicted position. And the weight is adjusted to achieve this objective.

With this method, the velocity of the motion also can be changed. This method can be verified when we test this model. Motion predictor predicts the fixed size of frames which is denser than the ground truth data. This means that if we change the amount of sampling ratio from motion predictor, the velocity of motion would be changed. If the sampling step size is big, the motion would be seemed to change rapidly. If the sampling step size is small, the motion would be seemed to change slowly. This method can be explained like this Figure 3.

\begin{figure}[h]
\begin{center}
\includegraphics [scale=0.33] {Images/upsample2.JPG}
\end{center}
\caption{Prediction by Up-sampling. $Left:$ Same motion $M_k$ is set to have the different number of frames. $Right:$ Sequence of shown frames at each row would be perceived to have motion like this.}
\label{fig:short}
\label{fig:onecol}
\end{figure}


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}




